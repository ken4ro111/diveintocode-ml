{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 論文読解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    "\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題\n",
    "\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "(4) RPNとは何か。\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 条件\n",
    "\n",
    "答える際は論文のどの部分からそれが分かるかを書く。\n",
    "\n",
    "必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "\n",
    "論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO\n",
    "\n",
    "http://mscoco.org/\n",
    "\n",
    "以下の８０種類のカテゴリーが設定されており、コンペやトレーニングのモデルとしてよく活用されている\n",
    "\n",
    "‘person‘, ‘bicycle‘, ‘car‘, ‘motorcycle‘, ‘airplane‘, ‘bus‘, ‘train‘, ‘truck’, ‘boat‘, ‘traffic light’, ‘fire hydrant’, ‘stop sign’, ‘parking meter’, ‘bench’, ‘bird‘, ‘cat‘, ‘dog‘, ‘horse‘, ‘sheep‘, ‘cow‘, ‘elephant’, ‘bear’, ‘zebra’, ‘giraffe’, ‘backpack’, ‘umbrella’, ‘handbag’, ‘tie’, ‘suitcase’, ‘frisbee’, ‘skis’, ‘snowboard’, ‘sports ball’, ‘kite’, ‘baseball bat’, ‘baseball glove’, ‘skateboard’, ‘surfboard’, ‘tennis racket’, ‘bottle‘, ‘wine glass’, ‘cup’, ‘fork’, ‘knife’, ‘spoon’, ‘bowl’, ‘banana’, ‘apple’, ‘sandwich’, ‘orange’, ‘broccoli’, ‘carrot’, ‘hot dog’, ‘pizza’, ‘donut’, ‘cake’, ‘chair‘, ‘couch‘, ‘potted plant‘, ‘bed’, ‘dining table‘, ‘toilet’, ‘tv‘, ‘laptop’, ‘mouse’, ‘remote’, ‘keyboard’, ‘cell phone’, ‘microwave’, ‘oven’, ‘toaster’, ‘sink’, ‘refrigerator’, ‘book’, ‘clock’, ‘vase’, ‘scissors’, ‘teddy bear’, ‘hair drier’, ‘toothbrush’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物体検出のタスクに対してもCNNのアルゴリズムを応用できないかと課題定義された。（CNNは非常に遅く、計算上非常に高価で、スライディングウィンドウ検出器によって生成された非常に多くのパッチでCNNを実行することは不可能だった）\n",
    "R-CNNではSelective Search（複数のスケールのウィンドウを調べて、\n",
    "テクスチャ、色、または強度を共有する隣接ピクセルを探し\n",
    "物体を識別します。）と呼ばれる物体候補アルゴリズムを使用し、この問題を改善"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast-R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-CNNはネットワークの順伝播計算を提案された物体領域ごとに行う必要があったため、検出速度が遅いという問題点があった。\n",
    "そこで１回計算した特徴マップを全ての提案された物体領域で再利用することで、R-CNNを高速化させたのがFast-R-CNN\n",
    "\n",
    "主に三つの層に分けられる。[Backbone] , [RPN] , [Head　：出力層]\n",
    "cis layerから「物体とみなすか否か」、reg layerから「オフセット」が得られる\n",
    "このcis layer ,reg layerそれぞれ損失を求め、それらを合算した損失をRPNの損失とする\n",
    "\n",
    "https://www.slideshare.net/windmdk/mask-rcnn\n",
    "https://qiita.com/shtmr/items/4283c851bc3d9721ed96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# non-maximum suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-maximum suppressionはIoU（領域の重なり具合を数値化したもの　IoU = (A^B) / (A∨B）)を利用して、同じクラスとして認識された重なっている状態の領域を抑制するためのアルゴリズムです\n",
    "IoU値の閾値を0.3という具合に定め、この閾値よりも大きいものを重複した物体領域の候補として外し、閾値に満たないものは、物体領域の候補として残すことになります。\n",
    "\n",
    "物体検出の結果として、検出対象物体を中心として複数のバウンディングボックスが検出されてしまうことがあるので、同一物体に複数のバウンディングボックスが検出されないようにするために、バウンディングボックスごとに検出の信頼度を表すスコアを計算し、局所的に最大スコアのバウンディングボックスのみを表示し、その他を表示しないように抑制する。その処理を **非最大値の抑制（non-maximum suppression　：　NMS）** と呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTER R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with ‘attention’ [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms for training both modules with features shared.\n",
    "\n",
    "Faster R-CNNと呼ばれるオブジェクト検出システムは、2つのモジュールで構成されています。最初のモジュールは、領域を提案する完全な畳み込みネットワークであり、2番目のモジュールは、提案された領域を使用する高速R-CNN検出器です[ 2 ]。システム全体は、オブジェクト検出用の単一の統合ネットワークです（図  2）。「注意」[ 31 ]メカニズムを備えたニューラルネットワークの最近人気のある用語を使用して、RPNモジュールはFast R-CNNモジュールにどこを見ればよいかを伝えます。セクション  3.1では、地域提案のためのネットワークの設計と特性を紹介します。セクション3.2では  、機能を共有して両方のモジュールをトレーニングするアルゴリズムを開発します。\n",
    "\n",
    "https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272https://www.youtube.com/watch?v=XGi-Mz3do2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoUとは重複する面積の割合を評価し、IoUが閾値0.7以上であれば物体、IoUが閾値0.3以下であれば非物体、それ以外のアンカーは評価対象外となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) 物体検出の分野にはどういった手法が存在したか。\n",
    "https://github.com/hoya012/deep_learning_object_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Proposals. There is a large literature on object\n",
    "proposal methods. Comprehensive surveys and comparisons of object proposal methods can be found in\n",
    "[19], [20], [21]. Widely used object proposal methods\n",
    "include those based on grouping super-pixels (e.g.,\n",
    "Selective Search [4], CPMC [22], MCG [23]) and those\n",
    "based on sliding windows (e.g., objectness in windows\n",
    "[24], EdgeBoxes [6]). Object proposal methods were\n",
    "adopted as external modules independent of the detectors (e.g., Selective Search [4] object detectors, RCNN [5], and Fast R-CNN [2]).\n",
    "\n",
    " - Selective Search\n",
    " - MCG\n",
    " - CPMC\n",
    "\n",
    "\n",
    "\n",
    "# ディープラーニングobject_detection\n",
    "- R-CNN\n",
    "- OverFeat\n",
    "- Fast R-CNN\n",
    "- Faster R-CNN\n",
    " \n",
    " \n",
    " \n",
    " - OHEM\n",
    " - YOLO v1\n",
    " - SSD\n",
    " - R-FCN\n",
    " - YOLO v2\n",
    " - FPN\n",
    " - RetinaNet\n",
    " - Mask R-CNN\n",
    " - YOLO v3\n",
    " - RefineDet\n",
    " - M2Det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Fasterとあるが、どういった仕組みで高速化したのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our object detection system, called Faster R-CNN, is\n",
    "composed of two modules. The first module is a deep\n",
    "fully convolutional network that proposes regions,\n",
    "and the second module is the Fast R-CNN detector [2]\n",
    "that uses the proposed regions. The entire system is a\n",
    "single, unified network for object detection (Figure 2).\n",
    "Using the recently popular terminology of neural\n",
    "networks with ‘attention’ [31] mechanisms, the RPN\n",
    "module tells the Fast R-CNN module where to look.\n",
    "In Section 3.1 we introduce the designs and properties\n",
    "of the network for region proposal. In Section 3.2 we\n",
    "develop algorithms\n",
    "\n",
    "Faster R-CNNと呼ばれるオブジェクト検出システムは２つのモジュールで構成されており、\n",
    "\n",
    " - 最初のモジュールは、RPN（領域を提案する畳み込みネットワーク\n",
    " - 二番目のモジュールは、RPNを使用するFaster R-CNN\n",
    " \n",
    "Fast R-CNNまでは物体領域抽出にSelective Searchと呼ばれるディープラーニング以前の手法をしようしていたため、抽出性能が低く候補が無数に抽出され、物体候補領域の抽出とその後の識別処理の計算コストが膨大となってしまっていた\n",
    " \n",
    " Faster R-CNNでは、物体候補領域の抽出をディープラーニングモデルに取り込み、Endo-to-Endoで学習、推定することにより高性能な物体候補領域の抽出を実現し、その結果高速かつ高性能な一般物体検出を実現した"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the one-stage and two-stage systems,\n",
    "we emulate the OverFeat system (and thus also circumvent other differences of implementation details) by\n",
    "one-stage Fast R-CNN. In this system, the “proposals”\n",
    "are dense sliding windows of 3 scales (128, 256, 512)\n",
    "and 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is\n",
    "trained to predict class-specific scores and regress box\n",
    "locations from these sliding windows. Because the\n",
    "OverFeat system adopts an image pyramid, we also\n",
    "evaluate using convolutional features extracted from\n",
    "5 scales. We use those 5 scales as in [1], [2].\n",
    "Table 10 compares the two-stage system and two\n",
    "variants of the one-stage system. Using the ZF model,\n",
    "the one-stage system has an mAP of 53.9%. This is\n",
    "lower than the two-stage system (58.7%) by 4.8%.\n",
    "\n",
    "1段階システムと2段階システムを比較するには、\n",
    "OverFeatシステムをエミュレートします（したがって、実装の詳細の他の違いを回避します）。\n",
    "1段階の高速R-CNN。 このシステムでは、「提案」\n",
    "3つのスケール（128、256、512）の密なスライディングウィンドウです。\n",
    "および3つのアスペクト比（1：1、1：2、2：1）。 高速R-CNNは\n",
    "クラス固有のスコアと回帰ボックスを予測するためのトレーニング\n",
    "これらのスライディングウィンドウからの場所。 なぜなら\n",
    "OverFeatシステムはイメージピラミッドを採用しており、\n",
    "から抽出された畳み込み特徴を使用して評価する\n",
    "5スケール。 [1]、[2]のように、これら5つのスケールを使用します。\n",
    "表10は、2段階システムと2段階システムを比較したものです\n",
    "1段階システムのバリアント。 ZFモデルを使用して、\n",
    "1段階システムのmAPは53.9％です。 これは\n",
    "2段階システム（58.7％）よりも4.8％低い。\n",
    "\n",
    "\n",
    "one-stage : クラスに特化した検出経路\n",
    "一つのネットワークで領域抽出とカテゴリ識別を同時に行う方法で、一般的に高速に動作する。代表的なモデルとして、SSD,Yolo,Retina Net\n",
    "\n",
    "two-stage : クラスに依存しない提案とクラスに特化した検出からなる経路 \n",
    "物体が写っている領域の候補を検出するRPNと領域候補のカテゴリを識別するネットワークを直接に実行する方法。代表的なモデルはFaster R-CNN,Mask R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) RPNとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, we introduce a Region Proposal Network (RPN) that shares full-image\n",
    "convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional\n",
    "network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to\n",
    "generate high-quality region proposals, which are used by Fast R-CNN for detection.\n",
    "\n",
    "BPNは、各位置でオブジェクトの境界とオブジェクトのスコアを同時に予測する完全な畳み込みネットワーク\n",
    "\n",
    "Firster　R-CNNが検出に使用する高品質のobject detectionを生成するためにエンドツーエンドでトレーニングされる\n",
    " \n",
    " 物体の候補領域を見つけるAncherと呼ばれる領域（物体の領域を示している領域）を生成する\n",
    " \n",
    " 候補の中からNMSなどを実施して、Region Proposal Networkでの物体の候補領域を抽出する\n",
    " NMS（Non-Maximum Suppression　　http://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/）\n",
    " とは重なりの多い領域を抑制し、領域の重複を防いでいる\n",
    " \n",
    "物体候補を出力するための2つの機能を持っています。1つ目は、枠内の画像が物体かどうかを表すスコアを計算する機能(cls layer)です。2つ目は、枠の概説矩形のスケールや位置を回帰により微調整する機能(reg layer)です。枠は、あらかじめ定義されたk個の外接矩形(Anchor)を用いて決定されます。このAnchor boxにさまざまな形、サイズを用意しておくとで多種多様な物体を検出できるようになるわけです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) RoIプーリングとは何か"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared computation of convolutions [9], [1], [29],\n",
    "[7], [2] has been attracting increasing attention for efficient, yet accurate, visual recognition. The OverFeat\n",
    "paper [9] computes convolutional features from an\n",
    "image pyramid for classification, localization, and detection. Adaptively-sized pooling (SPP) [1] on shared\n",
    "convolutional feature maps is developed for efficient\n",
    "region-based object detection [1], [30] and semantic\n",
    "segmentation [29]. Fast R-CNN [2] enables end-to-end\n",
    "detector training on shared convolutional features and\n",
    "shows compelling accuracy and speed.\n",
    "\n",
    "https://www.arxiv-vanity.com/papers/1512.04412/\n",
    "\n",
    "https://www.arxiv-vanity.com/papers/1703.06870/\n",
    "\n",
    "Given a box predicted by stage 1, we extract a feature of this box by Region-of-Interest (RoI) pooling [15, 9]. The purpose of RoI pooling is for producing a fixed-size feature from an arbitrary box, which is set as 14\n",
    "×\n",
    "14 at this stage. We append two extra fully-connected (fc) layers to this feature for each box. The first fc layer (with ReLU) reduces the dimension to 256, followed by the second fc layer that regresses a pixel-wise mask. This mask, of a pre-defined spatial resolution of \n",
    "m\n",
    "×\n",
    "m\n",
    " (we use \n",
    "m\n",
    "=\n",
    "28\n",
    "), is parameterized by an \n",
    "m\n",
    "2\n",
    "-dimensional vector. The second fc layer has \n",
    "m\n",
    "2\n",
    " outputs, each performing binary logistic regression to the ground truth mask.\n",
    " \n",
    " 物体領域の幅、高さ、また次元数も異なるため後の違った次元数が入力できない全結層直接利用できないことから、次元数の異なる特徴マップを一定の大きさのベクトルに変換するRoiプーリングを使用する。\n",
    "\n",
    "任意サイズの領域をプーリングして、固定次元の出力をする\n",
    "物体検出の領域は非常に可変長なので、得られた領域を次の分類ネットワークで処理するために、固定次元に縮小する処理を行う\n",
    "不均一なサイズの入力に対して最大プールを実行して、固定サイズの特徴マップ（例えば14　* 14）を得ること。画像の多くの領域候補が必ず重複し、同じCNN計算を何度も実行していたので、画像を一回のみCNN実行し、領域間で計算結果を共有した \n",
    "時間の大幅な短縮につながった\n",
    "\n",
    "\n",
    "RoIプーリングの目的は、この段階で7 \n",
    "×\n",
    " 7に設定されている任意のボックスから固定サイズの機能を作成することです。ボックスごとに、この機能に2つの追加の完全接続（fc）レイヤーを追加します。最初のfcレイヤー（ReLUを使用）は寸法を256に縮小し、次にピクセル単位のマスクを回帰する2番目のfcレイヤーが続きます。このマスクは、事前に定義された\n",
    "m \n",
    "× \n",
    "mの\n",
    "空間解像度（\n",
    "m \n",
    "= \n",
    "28\n",
    "を使用）であり、\n",
    "m \n",
    "2\n",
    "次元ベクトルによってパラメーター化されます。2番目のfc層には\n",
    "m\n",
    "2つの\n",
    "出力。それぞれがグラウンドトゥルースマスクに対してバイナリロジスティック回帰を実行します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (6) Anchorのサイズはどうするのが適切か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each sliding-window location, we simultaneously\n",
    "predict multiple region proposals, where the number\n",
    "of maximum possible proposals for each location is\n",
    "denoted as k. So the reg layer has 4k outputs encoding\n",
    "the coordinates of k boxes, and the cls layer outputs\n",
    "2k scores that estimate probability of object or not\n",
    "object for each proposal4\n",
    ". The k proposals are parameterized relative to k reference boxes, which we call　anchors. An anchor is centered at the sliding window\n",
    "in question, and is associated with a scale and aspect\n",
    "ratio (Figure 3, left). By default we use 3 scales and\n",
    "3 aspect ratios, yielding k = 9 anchors at each sliding\n",
    "position. For a convolutional feature map of a size\n",
    "W × H (typically ∼2,400), there are W Hk anchors in\n",
    "total.\n",
    "\n",
    "\n",
    "各sliding-windowで複数の地域提案を同時に予測する\n",
    "\n",
    "各場所の可能な最大提案の数は　**k**　で表される\n",
    "\n",
    "デフォルトでは　**k = 9**　でアンカーを生成する。そのkを中心としてアンカーボックスを生成し（k = 9ならば９つのアンカーボックス）\n",
    "元画像の物体領域（ground truth boxes）\n",
    "\n",
    "さまざまな形、サイズを用意しておくとで多種多様な物体を検出できる（**ただしボックスの面積を等しくなければならない**）\n",
    "\n",
    "スケール　：　128,256,512ピクセルなど\n",
    "\n",
    "ボックスの縦横比　アスペクト比　：　1:2 1:1 2:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we evaluate our Faster R-CNN system. Using\n",
    "the COCO training set to train, Faster R-CNN has\n",
    "42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the\n",
    "COCO test-dev set. This is 2.8% higher for mAP@0.5\n",
    "and 2.2% higher for mAP@[.5, .95] than the Fast RCNN counterpart under the same protocol (Table 11).\n",
    "This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds. Using the COCO trainval set to train, Faster RCNN has 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on\n",
    "the COCO test-dev set. Figure 6 shows some results\n",
    "on the MS COCO test-dev set\n",
    "\n",
    "より高速なR-CNNシステムを評価します。COCOトレーニングセットを使用してトレーニングすると、Faster R-CNNはCOCOテスト開発セットで42.1％mAP@0.5 および21.5％mAP @ [.5、.95]を持ちます。これは2.8％高いmAP@0.5 および2.2％より高いのためのマップ@ [0.5、0.95]同じプロトコル（表の下の高速R-CNN対応物より  XI）。これは、RPNが高いIoUしきい値でのローカライズの精度を向上させるために優れたパフォーマンスを発揮することを示しています。COCO trainvalセットを使用してトレーニングを行うと、Faster R-CNNはCOCO test-devセットで42.7％mAP@0.5 および21.9％mAP @ [.5、.95]になります。図  6は、MS COCO test-devセットのいくつかの結果を示しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
